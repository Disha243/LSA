{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of FinalLSAProject.ipynb","provenance":[{"file_id":"1IvTUolRR1AbwFsPt0fPb9p7MG7bIc_iq","timestamp":1604166417539}],"authorship_tag":"ABX9TyMtA5ewC4CMC1NN1RibBNe1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"XltzluG-iJG2"},"source":["import os\n","import pickle\n","import json\n","import random\n","import logging\n","import numpy as np\n","from itertools import chain\n","import torch\n","import torch.nn as nn\n","import torchvision\n","from torch.autograd import Variable\n","from torch.utils.data import Dataset, DataLoader\n","import torchio\n","from tqdm import tqdm\n","import sys\n","import matplotlib.pyplot as plt\n","from torch.optim import Adam\n","from torch import nn, optim\n","from torch.optim.lr_scheduler import StepLR\n","import torch.distributions as dist\n","import math\n","import import_ipynb\n","from functools import reduce\n","from operator import mul\n","from typing import List\n","from typing import Optional\n","from torchsummary import summary\n","import torch.nn.functional as F\n","\n","training_data = r\"/content/drive/My Drive/Colab Notebooks/Anomaly_Detection/LSA/data/mood.h5\"\n","validation_data = r\"/content/drive/My Drive/Colab Notebooks/Anomaly_Detection/LSA/data/MOOD_toytest_brain.h5\"\n","log_path = r\"/content/drive/My Drive/Colab Notebooks/Anomaly_Detection/LSA/log\"\n","save_path = r\"/content/drive/My Drive/Colab Notebooks/Anomaly_Detection/LSA/save_dir\"\n","\n","imgsh5_train = r\"/content/drive/My Drive/Colab Notebooks/Anomaly_Detection/LSA/data/mood.h5\"\n","imgsh5_val = r\"/content/drive/My Drive/Colab Notebooks/Anomaly_Detection/LSA/data/MOOD_toytest_brain.h5\"\n","\n","device = torch.cuda.set_device(0)\n","num_workers=0\n","mood_region='brain'\n","useCuda=True\n","do_val=True\n","gpuID=\"0\"\n","seed = 1701\n","trainID=\"Anomaly_ModenaVAE2D\"\n","batch_size = 2\n","num_epochs = 250\n","lr = 1e-4\n","patch_size=(256,256,1) #Set it to None if not desired\n","patchQ_len = 512\n","patches_per_volume = 256\n","amp_level = 'O1'    \n","log_freq = 10 \n","preload_h5 = True\n","indicesOfImgVols = [1,2]\n","\n","IsVAE=True\n","input_shape=(256,256,256)\n","code_length=64\n","cpd_channels=100 \n","n_starting_features=32  #32\n","n_channels=1\n","out_sigmoid=True\n","AutoregLoss_weight=1 #weight of the autoregression loss.\n","ce_factor=0.5\n","beta=0.01\n","vae_loss_ema = 1\n","theta = 1\n","use_geco=False\n","\n","mood_region='brain'\n","useCuda=True\n","checkpoint2load = None\n","random.seed(seed)\n","os.environ['PYTHONHASHSEED'] = str(seed)\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed(seed)\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False\n","\n","from Data import MoodTrainSet, MoodValSet\n","\n","trainset = MoodTrainSet(indices=indicesOfImgVols, region=mood_region, data_path=training_data, lazypatch=True if patch_size else False, preload=preload_h5)\n","valset = MoodValSet(data_path=validation_data, lazypatch=True if patch_size else False, preload=preload_h5)\n","\n","if patch_size:\n","  input_shape = tuple(x for x in patch_size if x!=1)\n","  trainset = torchio.data.Queue(\n","                  subjects_dataset = trainset,\n","                  max_length = patchQ_len,\n","                  samples_per_volume = patches_per_volume,\n","                  sampler = torchio.data.UniformSampler(patch_size=patch_size),\n","                  # num_workers = num_workers\n","                  )\n","  valset = torchio.data.Queue(\n","                  subjects_dataset = valset,\n","                  max_length = patchQ_len,\n","                  samples_per_volume = patches_per_volume,\n","                  sampler = torchio.data.UniformSampler(patch_size=patch_size),\n","                  # num_workers = num_workers\n","                  )\n","\n","train_loader = DataLoader(dataset=trainset,batch_size=batch_size,shuffle=False, num_workers=num_workers)\n","val_loader = None if (valset is None) or (not do_val) else DataLoader(dataset=valset,batch_size=batch_size,shuffle=False, num_workers=num_workers)\n","\n","trainset = MoodTrainSet(indices=indicesOfImgVols, region=mood_region, data_path=training_data, lazypatch=True if patch_size else False, preload=preload_h5)\n","valset = MoodValSet(data_path=validation_data, lazypatch=True if patch_size else False, preload=preload_h5)\n","\n","if patch_size:\n","  input_shape = tuple(x for x in patch_size if x!=1)\n","  trainset = torchio.data.Queue(\n","                  subjects_dataset = trainset,\n","                  max_length = patchQ_len,\n","                  samples_per_volume = patches_per_volume,\n","                  sampler = torchio.data.UniformSampler(patch_size=patch_size),\n","                  # num_workers = num_workers\n","                  )\n","  valset = torchio.data.Queue(\n","                  subjects_dataset = valset,\n","                  max_length = patchQ_len,\n","                  samples_per_volume = patches_per_volume,\n","                  sampler = torchio.data.UniformSampler(patch_size=patch_size),\n","                  # num_workers = num_workers\n","                  )\n","\n","train_loader = DataLoader(dataset=trainset,batch_size=batch_size,shuffle=False, num_workers=num_workers)\n","val_loader = None if (valset is None) or (not do_val) else DataLoader(dataset=valset,batch_size=batch_size,shuffle=False, num_workers=num_workers)\n","\n","\n","class BaseModule(nn.Module):\n","    \"\"\"\n","    Implements the basic module.\n","    All other modules inherit from this one\n","    \"\"\"\n","    def load_w(self, checkpoint_path):\n","        # type: (str) -> None\n","        \"\"\"\n","        Loads a checkpoint into the state_dict.\n","\n","        :param checkpoint_path: the checkpoint file to be loaded.\n","        \"\"\"\n","        self.load_state_dict(torch.load(checkpoint_path))\n","\n","    def __repr__(self):\n","        # type: () -> str\n","        \"\"\"\n","        String representation\n","        \"\"\"\n","        good_old = super(BaseModule, self).__repr__()\n","        addition = 'Total number of parameters: {:,}'.format(self.n_parameters)\n","\n","        return good_old + '\\n' + addition\n","\n","    def __call__(self, *args, **kwargs):\n","        return super(BaseModule, self).__call__(*args, **kwargs)\n","\n","    @property\n","    def n_parameters(self):\n","        # type: () -> int\n","        \"\"\"\n","        Number of parameters of the model.\n","        \"\"\"\n","        n_parameters = 0\n","        for p in self.parameters():\n","            if hasattr(p, 'mask'):\n","                n_parameters += torch.sum(p.mask).item()\n","            else:\n","                n_parameters += reduce(mul, p.shape)\n","        return int(n_parameters)\n","\n","def residual_op(x, functions, bns, activation_fn):\n","    # type: (torch.Tensor, List[Module, Module, Module], List[Module, Module, Module], Module) -> torch.Tensor\n","    \"\"\"\n","    Implements a global residual operation.\n","\n","    :param x: the input tensor.\n","    :param functions: a list of functions (nn.Modules).\n","    :param bns: a list of optional batch-norm layers.\n","    :param activation_fn: the activation to be applied.\n","    :return: the output of the residual operation.\n","    \"\"\"\n","    f1, f2, f3 = functions\n","    bn1, bn2, bn3 = bns\n","\n","    assert len(functions) == len(bns) == 3\n","    assert f1 is not None and f2 is not None\n","    assert not (f3 is None and bn3 is not None)\n","\n","    # A-branch\n","    ha = x\n","    ha = f1(ha)\n","    if bn1 is not None:\n","        ha = bn1(ha)\n","    ha = activation_fn(ha)\n","\n","    ha = f2(ha)\n","    if bn2 is not None:\n","        ha = bn2(ha)\n","\n","    # B-branch\n","    hb = x\n","    if f3 is not None:\n","        hb = f3(hb)\n","    if bn3 is not None:\n","        hb = bn3(hb)\n","\n","    # Residual connection\n","    out = ha + hb\n","    return activation_fn(out)\n","\n","\n","class BaseBlock(BaseModule):\n","    \"\"\" Base class for all blocks. \"\"\"\n","    def __init__(self, channel_in, channel_out, activation_fn, use_bn=True, use_bias=False):\n","        # type: (int, int, Module, bool, bool) -> None\n","        \"\"\"\n","        Class constructor.\n","\n","        :param channel_in: number of input channels.\n","        :param channel_out: number of output channels.\n","        :param activation_fn: activation to be employed.\n","        :param use_bn: whether or not to use batch-norm.\n","        :param use_bias: whether or not to use bias.\n","        \"\"\"\n","        super(BaseBlock, self).__init__()\n","\n","        assert not (use_bn and use_bias), 'Using bias=True with batch_normalization is forbidden.'\n","\n","        self._channel_in = channel_in\n","        self._channel_out = channel_out\n","        self._activation_fn = activation_fn\n","        self._use_bn = use_bn\n","        self._bias = use_bias\n","\n","    def get_bn(self):\n","        # type: () -> Optional[Module]\n","        \"\"\"\n","        Returns batch norm layers, if needed.\n","        :return: batch norm layers or None\n","        \"\"\"\n","        return nn.BatchNorm2d(num_features=self._channel_out) if self._use_bn else None\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Abstract forward function. Not implemented.\n","        \"\"\"\n","        raise NotImplementedError\n","\n","\n","class DownsampleBlock(BaseBlock):\n","    \"\"\" Implements a Downsampling block for images (Fig. 1ii). \"\"\"\n","    def __init__(self, channel_in, channel_out, activation_fn, use_bn=True, use_bias=False):\n","        # type: (int, int, Module, bool, bool) -> None\n","        \"\"\"\n","        Class constructor.\n","\n","        :param channel_in: number of input channels.\n","        :param channel_out: number of output channels.\n","        :param activation_fn: activation to be employed.\n","        :param use_bn: whether or not to use batch-norm.\n","        :param use_bias: whether or not to use bias.\n","        \"\"\"\n","        super(DownsampleBlock, self).__init__(channel_in, channel_out, activation_fn, use_bn, use_bias)\n","\n","        # Convolutions\n","        self.conv1a = nn.Conv2d(in_channels=channel_in, out_channels=channel_out, kernel_size=3,\n","                                padding=1, stride=2, bias=use_bias)\n","        self.conv1b = nn.Conv2d(in_channels=channel_out, out_channels=channel_out, kernel_size=3,\n","                                padding=1, stride=1, bias=use_bias)\n","        self.conv2a = nn.Conv2d(in_channels=channel_in, out_channels=channel_out, kernel_size=1,\n","                                padding=0, stride=2, bias=use_bias)\n","\n","        # Batch Normalization layers\n","        self.bn1a = self.get_bn()\n","        self.bn1b = self.get_bn()\n","        self.bn2a = self.get_bn()\n","\n","    def forward(self, x):\n","        # type: (torch.Tensor) -> torch.Tensor\n","        \"\"\"\n","        Forward propagation.\n","        :param x: the input tensor\n","        :return: the output tensor\n","        \"\"\"\n","        return residual_op(\n","            x,\n","            functions=[self.conv1a, self.conv1b, self.conv2a],\n","            bns=[self.bn1a, self.bn1b, self.bn2a],\n","            activation_fn=self._activation_fn\n","        )\n","\n","\n","class UpsampleBlock(BaseBlock):\n","    \"\"\" Implements a Upsampling block for images (Fig. 1ii). \"\"\"\n","    def __init__(self, channel_in, channel_out, activation_fn, use_bn=True, use_bias=False):\n","        # type: (int, int, Module, bool, bool) -> None\n","        \"\"\"\n","        Class constructor.\n","\n","        :param channel_in: number of input channels.\n","        :param channel_out: number of output channels.\n","        :param activation_fn: activation to be employed.\n","        :param use_bn: whether or not to use batch-norm.\n","        :param use_bias: whether or not to use bias.\n","        \"\"\"\n","        super(UpsampleBlock, self).__init__(channel_in, channel_out, activation_fn, use_bn, use_bias)\n","\n","        # Convolutions\n","        self.conv1a = nn.ConvTranspose2d(channel_in, channel_out, kernel_size=5,\n","                                         padding=2, stride=2, output_padding=1, bias=use_bias)\n","        self.conv1b = nn.Conv2d(in_channels=channel_out, out_channels=channel_out, kernel_size=3,\n","                                padding=1, stride=1, bias=use_bias)\n","        self.conv2a = nn.ConvTranspose2d(channel_in, channel_out, kernel_size=1,\n","                                         padding=0, stride=2, output_padding=1, bias=use_bias)\n","\n","        # Batch Normalization layers\n","        self.bn1a = self.get_bn()\n","        self.bn1b = self.get_bn()\n","        self.bn2a = self.get_bn()\n","\n","    def forward(self, x):\n","        # type: (torch.Tensor) -> torch.Tensor\n","        \"\"\"\n","        Forward propagation.\n","        :param x: the input tensor\n","        :return: the output tensor\n","        \"\"\"\n","        return residual_op(\n","            x,\n","            functions=[self.conv1a, self.conv1b, self.conv2a],\n","            bns=[self.bn1a, self.bn1b, self.bn2a],\n","            activation_fn=self._activation_fn\n","        )\n","\n","\n","class ResidualBlock(BaseBlock):\n","    \"\"\" Implements a Residual block for images (Fig. 1ii). \"\"\"\n","    def __init__(self, channel_in, channel_out, activation_fn, use_bn=True, use_bias=False):\n","        # type: (int, int, Module, bool, bool) -> None\n","        \"\"\"\n","        Class constructor.\n","\n","        :param channel_in: number of input channels.\n","        :param channel_out: number of output channels.\n","        :param activation_fn: activation to be employed.\n","        :param use_bn: whether or not to use batch-norm.\n","        :param use_bias: whether or not to use bias.\n","        \"\"\"\n","        super(ResidualBlock, self).__init__(channel_in, channel_out, activation_fn, use_bn, use_bias)\n","\n","        # Convolutions\n","        self.conv1 = nn.Conv2d(in_channels=channel_in, out_channels=channel_out, kernel_size=3,\n","                               padding=1, stride=1, bias=use_bias)\n","        self.conv2 = nn.Conv2d(in_channels=channel_out, out_channels=channel_out, kernel_size=3,\n","                               padding=1, stride=1, bias=use_bias)\n","\n","        # Batch Normalization layers\n","        self.bn1 = self.get_bn()\n","        self.bn2 = self.get_bn()\n","\n","    def forward(self, x):\n","        # type: (torch.Tensor) -> torch.Tensor\n","        \"\"\"\n","        Forward propagation.\n","        :param x: the input tensor\n","        :return: the output tensor\n","        \"\"\"\n","        return residual_op(\n","            x,\n","            functions=[self.conv1, self.conv2, None],\n","            bns=[self.bn1, self.bn2, None],\n","            activation_fn=self._activation_fn\n","        )\n","\n","class MaskedFullyConnection(BaseModule, nn.Linear):\n","    \"\"\"\n","    Implements a Masked Fully Connection layer (MFC, Eq. 6).\n","    This is the autoregressive layer employed for the estimation of\n","    densities of image feature vectors.\n","    \"\"\"\n","    def __init__(self, mask_type, in_channels, out_channels, *args, **kwargs):\n","        \"\"\"\n","        Class constructor.\n","\n","        :param mask_type: type of autoregressive layer, either `A` or `B`.\n","        :param in_channels: number of input channels.\n","        :param out_channels: number of output channels.\n","        \"\"\"\n","        self.mask_type = mask_type\n","        self.in_channels = in_channels\n","        self.out_channels = out_channels\n","        super(MaskedFullyConnection, self).__init__(*args, **kwargs)\n","\n","        assert mask_type in ['A', 'B']\n","        self.register_buffer('mask', self.weight.data.clone())\n","\n","        # Build mask\n","        self.mask.fill_(0)\n","        for f in range(0 if mask_type == 'B' else 1, self.out_features // self.out_channels):\n","            start_row = f*self.out_channels\n","            end_row = (f+1)*self.out_channels\n","            start_col = 0\n","            end_col = f*self.in_channels if mask_type == 'A' else (f+1)*self.in_channels\n","            if start_col != end_col:\n","                self.mask[start_row:end_row, start_col:end_col] = 1\n","\n","        self.weight.mask = self.mask\n","\n","    def forward(self, x):\n","        # type: (torch.Tensor) -> torch.Tensor\n","        \"\"\"\n","        Forward propagation.\n","\n","        :param x: the input tensor.\n","        :return: the output of a MFC manipulation.\n","        \"\"\"\n","\n","        # Reshape\n","        x = torch.transpose(x, 1, 2).contiguous()\n","        x = x.view(len(x), -1)\n","\n","        # Mask weights and call fully connection\n","        self.weight.data *= self.mask\n","        o = super(MaskedFullyConnection, self).forward(x)\n","\n","        # Reshape again\n","        o = o.view(len(o), -1, self.out_channels)\n","        o = torch.transpose(o, 1, 2).contiguous()\n","\n","        return o\n","\n","    def __repr__(self):\n","        # type: () -> str\n","        \"\"\"\n","        String representation.\n","        \"\"\"\n","        return self.__class__.__name__ + '(' \\\n","               + 'mask_type=' + str(self.mask_type) \\\n","               + ', in_features=' + str(self.in_features // self.in_channels) \\\n","               + ', out_features=' + str(self.out_features // self.out_channels)\\\n","               + ', in_channels=' + str(self.in_channels) \\\n","               + ', out_channels=' + str(self.out_channels) \\\n","               + ', n_params=' + str(self.n_parameters) + ')'\n","\n","\n","class Estimator1D(BaseModule):\n","    \"\"\"\n","    Implements an estimator for 1-dimensional vectors.\n","    1-dimensional vectors arise from the encoding of images.\n","    This module is employed in MNIST and CIFAR10 LSA models.\n","    Takes as input a latent vector and outputs cpds for each variable.\n","    \"\"\"\n","    def __init__(self, code_length, fm_list, cpd_channels):\n","        # type: (int, List[int], int) -> None\n","        \"\"\"\n","        Class constructor.\n","\n","        :param code_length: the dimensionality of latent vectors.\n","        :param fm_list: list of channels for each MFC layer.\n","        :param cpd_channels: number of bins in which the multinomial works.\n","        \"\"\"\n","        super(Estimator1D, self).__init__()\n","\n","        self.code_length = code_length\n","        self.fm_list = fm_list\n","        self.cpd_channels = cpd_channels\n","\n","        activation_fn = nn.LeakyReLU()\n","\n","        # Add autoregressive layers\n","        layers_list = []\n","        mask_type = 'A'\n","        fm_in = 1\n","        for l in range(0, len(fm_list)):\n","\n","            fm_out = fm_list[l]\n","            layers_list.append(\n","                MaskedFullyConnection(mask_type=mask_type,\n","                                      in_features=fm_in * code_length,\n","                                      out_features=fm_out * code_length,\n","                                      in_channels=fm_in, out_channels=fm_out)\n","            )\n","            layers_list.append(activation_fn)\n","\n","            mask_type = 'B'\n","            fm_in = fm_list[l]\n","\n","        # Add final layer providing cpd params\n","        layers_list.append(\n","            MaskedFullyConnection(mask_type=mask_type,\n","                                  in_features=fm_in * code_length,\n","                                  out_features=cpd_channels * code_length,\n","                                  in_channels=fm_in,\n","                                  out_channels=cpd_channels))\n","\n","        self.layers = nn.Sequential(*layers_list)\n","\n","    def forward(self, x):\n","        # type: (torch.Tensor) -> torch.Tensor\n","        \"\"\"\n","        Forward propagation.\n","\n","        :param x: the batch of latent vectors.\n","        :return: the batch of CPD estimates.\n","        \"\"\"\n","        h = torch.unsqueeze(x, dim=1)  # add singleton channel dim\n","        h = self.layers(h)\n","        o = h\n","\n","        return o\n","\n","def get_square_mask(data_shape, square_size, n_squares, noise_val=(0, 0), channel_wise_n_val=False, square_pos=None):\n","    \"\"\"Returns a 'mask' with the same size as the data, where random squares are != 0\n","\n","    Args:\n","        data_shape ([tensor]): [data_shape to determine the shape of the returned tensor]\n","        square_size ([tuple]): [int/ int tuple (min_size, max_size), determining the min and max squear size]\n","        n_squares ([type]): [int/ int tuple (min_number, max_number), determining the min and max number of squares]\n","        noise_val (tuple, optional): [int/ int tuple (min_val, max_val), determining the min and max value given in the \n","                                        squares, which habe the value != 0 ]. Defaults to (0, 0).\n","        channel_wise_n_val (bool, optional): [Use a different value for each channel]. Defaults to False.\n","        square_pos ([type], optional): [Square position]. Defaults to None.\n","    \"\"\"\n","\n","    def mask_random_square(img_shape, square_size, n_val, channel_wise_n_val=False, square_pos=None):\n","        \"\"\"Masks (sets = 0) a random square in an image\"\"\"\n","\n","        img_h = img_shape[-2]\n","        img_w = img_shape[-1]\n","\n","        img = np.zeros(img_shape)\n","\n","        if square_pos is None:\n","            w_start = np.random.randint(0, img_w - square_size)\n","            h_start = np.random.randint(0, img_h - square_size)\n","        else:\n","            pos_wh = square_pos[np.random.randint(0, len(square_pos))]\n","            w_start = pos_wh[0]\n","            h_start = pos_wh[1]\n","\n","        if img.ndim == 2:\n","            rnd_n_val = get_range_val(n_val)\n","            img[h_start : (h_start + square_size), w_start : (w_start + square_size)] = rnd_n_val\n","        elif img.ndim == 3:\n","            if channel_wise_n_val:\n","                for i in range(img.shape[0]):\n","                    rnd_n_val = get_range_val(n_val)\n","                    img[i, h_start : (h_start + square_size), w_start : (w_start + square_size)] = rnd_n_val\n","            else:\n","                rnd_n_val = get_range_val(n_val)\n","                img[:, h_start : (h_start + square_size), w_start : (w_start + square_size)] = rnd_n_val\n","        elif img.ndim == 4:\n","            if channel_wise_n_val:\n","                for i in range(img.shape[0]):\n","                    rnd_n_val = get_range_val(n_val)\n","                    img[:, i, h_start : (h_start + square_size), w_start : (w_start + square_size)] = rnd_n_val\n","            else:\n","                rnd_n_val = get_range_val(n_val)\n","                img[:, :, h_start : (h_start + square_size), w_start : (w_start + square_size)] = rnd_n_val\n","\n","        return img\n","\n","    def mask_random_squares(img_shape, square_size, n_squares, n_val, channel_wise_n_val=False, square_pos=None):\n","        \"\"\"Masks a given number of squares in an image\"\"\"\n","        img = np.zeros(img_shape)\n","        for i in range(n_squares):\n","            img = mask_random_square(\n","                img_shape, square_size, n_val, channel_wise_n_val=channel_wise_n_val, square_pos=square_pos\n","            )\n","        return img\n","\n","    ret_data = np.zeros(data_shape)\n","    for sample_idx in range(data_shape[0]):\n","        # rnd_n_val = get_range_val(noise_val)\n","        rnd_square_size = get_range_val(square_size)\n","        rnd_n_squares = get_range_val(n_squares)\n","\n","        ret_data[sample_idx] = mask_random_squares(\n","            data_shape[1:],\n","            square_size=rnd_square_size,\n","            n_squares=rnd_n_squares,\n","            n_val=noise_val,\n","            channel_wise_n_val=channel_wise_n_val,\n","            square_pos=square_pos,\n","        )\n","\n","    return ret_data\n","\n","def get_range_val(value, rnd_type=\"uniform\"):\n","    if isinstance(value, (list, tuple, np.ndarray)):\n","        if len(value) == 2:\n","            if value[0] == value[1]:\n","                n_val = value[0]\n","            else:\n","                orig_type = type(value[0])\n","                if rnd_type == \"uniform\":\n","                    n_val = random.uniform(value[0], value[1])\n","                elif rnd_type == \"normal\":\n","                    n_val = random.normalvariate(value[0], value[1])\n","                n_val = orig_type(n_val)\n","        elif len(value) == 1:\n","            n_val = value[0]\n","        else:\n","            raise RuntimeError(\"value must be either a single vlaue or a list/tuple of len 2\")\n","        return n_val\n","    else:\n","        return value\n","\n","\n","class Encoder(BaseModule):\n","    \"\"\"\n","    MOOD model encoder based on CIFAR10.\n","    \"\"\"\n","    def __init__(self, input_shape, code_length, n_starting_features=32, conv=nn.Conv2d):\n","        # type: (Tuple[int, int, int], int) -> None\n","        \"\"\"\n","        Class constructor:\n","\n","        :param input_shape: the shape of CIFAR10 samples.\n","        :param code_length: the dimensionality of latent vectors.\n","        \"\"\"\n","        super(Encoder, self).__init__()\n","\n","        self.input_shape = input_shape\n","        self.code_length = code_length\n","\n","        if len(input_shape) == 3:\n","          c, h, w = input_shape\n","          self.deepest_shape = (n_starting_features*8, h // 8, w // 8)\n","          \n","\n","        activation_fn = nn.LeakyReLU()\n","\n","        # Convolutional network\n","        self.conv = nn.Sequential(\n","            conv(in_channels=c, out_channels=n_starting_features, kernel_size=3, bias=False),\n","            activation_fn,\n","            ResidualBlock(channel_in=n_starting_features, channel_out=n_starting_features, activation_fn=activation_fn),\n","            DownsampleBlock(channel_in=n_starting_features, channel_out=n_starting_features*2, activation_fn=activation_fn),\n","            DownsampleBlock(channel_in=n_starting_features*2, channel_out=n_starting_features*4, activation_fn=activation_fn),\n","            DownsampleBlock(channel_in=n_starting_features*4, channel_out=n_starting_features*8, activation_fn=activation_fn),\n","        )\n","\n","        # FC network\n","        self.fc = nn.Sequential(\n","            nn.Linear(in_features=reduce(mul, self.deepest_shape), out_features=n_starting_features*8),\n","            nn.BatchNorm1d(num_features=n_starting_features*8),\n","            activation_fn,\n","            nn.Linear(in_features=n_starting_features*8, out_features=code_length),\n","            nn.Sigmoid()\n","        )\n","    def forward(self, x):\n","        # types: (torch.Tensor) -> torch.Tensor\n","        \"\"\"\n","        Forward propagation.\n","\n","        :param x: the input batch of images.\n","        :return: the batch of latent vectors.\n","        \"\"\"\n","        h = x\n","        h = self.conv(h)\n","        h = h.view(len(h), -1)\n","        o = self.fc(h)\n","\n","        return o\n","\n","\n","class Decoder(BaseModule):\n","    \"\"\"\n","    CIFAR10 model decoder.\n","    \"\"\"\n","    def __init__(self, code_length, deepest_shape, output_shape, n_starting_features=32, n_channels=3, conv=nn.Conv3d):\n","        # type: (int, Tuple[int, int, int], Tuple[int, int, int]) -> None\n","        \"\"\"\n","        Class constructor.\n","\n","        :param code_length: the dimensionality of latent vectors.\n","        :param deepest_shape: the dimensionality of the encoder's deepest convolutional map.\n","        :param output_shape: the shape of CIFAR10 samples.\n","        \"\"\"\n","        super(Decoder, self).__init__()\n","\n","        self.code_length = code_length\n","        self.deepest_shape = deepest_shape\n","        self.output_shape = output_shape\n","\n","        activation_fn = nn.LeakyReLU()\n","        \n","\n","\n","        # FC network\n","        self.fc = nn.Sequential(\n","            nn.Linear(in_features=code_length, out_features=n_starting_features*8),\n","            nn.BatchNorm1d(num_features=n_starting_features*8),\n","            activation_fn,\n","            nn.Linear(in_features=n_starting_features*8, out_features=reduce(mul, deepest_shape)),\n","            nn.BatchNorm1d(num_features=reduce(mul, deepest_shape)),\n","            activation_fn\n","        )\n","\n","        # Convolutional network\n","        self.conv = nn.Sequential(\n","            UpsampleBlock(channel_in=n_starting_features*8, channel_out=n_starting_features*4, activation_fn=activation_fn),\n","            UpsampleBlock(channel_in=n_starting_features*4, channel_out=n_starting_features*2, activation_fn=activation_fn),\n","            UpsampleBlock(channel_in=n_starting_features*2, channel_out=n_starting_features, activation_fn=activation_fn),\n","            ResidualBlock(channel_in=n_starting_features, channel_out=n_starting_features, activation_fn=activation_fn),\n","            conv(in_channels=n_starting_features, out_channels=n_channels, kernel_size=1, bias=False)\n","        )\n","\n","    def forward(self, x):\n","        # types: (torch.Tensor) -> torch.Tensor\n","        \"\"\"\n","        Forward propagation.\n","\n","        :param x: the batch of latent vectors.\n","        :return: the batch of reconstructions.\n","        \"\"\"\n","        h = x\n","        h = self.fc(h)\n","        h = h.view(len(h), *self.deepest_shape)\n","        h = self.conv(h)\n","        o = h\n","\n","        return o\n","\n","\n","class LSAMOOD(BaseModule):\n","    \"\"\"\n","    LSA model for CIFAR10 one-class classification.\n","    \"\"\"\n","    def __init__(self,  input_shape, code_length, cpd_channels, d=3, n_starting_features=32, n_channels=3, out_sigmoid=True, vae_mode=False):\n","        # type: (Tuple[int, int, int], int, int) -> None\n","        \"\"\"\n","        Class constructor.\n","\n","        :param input_shape: the shape of CIFAR10 samples.\n","        :param code_length: the dimensionality of latent vectors.\n","        :param cpd_channels: number of bins in which the multinomial works.\n","        \"\"\"\n","        super(LSAMOOD, self).__init__()\n","\n","        if type(input_shape) is tuple:\n","            d=len(input_shape)-1\n","\n","        self.input_shape = input_shape\n","        self.code_length = code_length\n","        self.out_sigmoid = out_sigmoid\n","        self.vae_mode = vae_mode\n","        \n","        self.d = d\n","        conv = nn.Conv2d\n","        \n","\n","        # Build encoder\n","        self.encoder = Encoder(\n","            input_shape=input_shape,\n","            code_length=code_length*2 if vae_mode else code_length,\n","            n_starting_features=n_starting_features,\n","            conv=conv\n","        )\n","\n","        # Build decoder\n","        self.decoder = Decoder(\n","            code_length=code_length,\n","            deepest_shape=self.encoder.deepest_shape,\n","            output_shape=input_shape,\n","            n_starting_features=n_starting_features,\n","            n_channels=n_channels,\n","            conv=conv\n","        )\n","\n","        if not vae_mode:\n","            # Build estimator\n","            self.estimator = Estimator1D(\n","                code_length=code_length,\n","                fm_list=[n_starting_features, n_starting_features, n_starting_features, n_starting_features],\n","                cpd_channels=cpd_channels\n","            )\n","\n","    def forward(self, x, reparam=True):\n","        # type: (torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]\n","        \"\"\"\n","        Forward propagation.\n","\n","        :param x: the input batch of images.\n","        :return: a tuple of torch.Tensors holding reconstructions, latent vectors and CPD estimates.\n","        \"\"\"\n","        h = x\n","\n","        # Produce representations\n","        z = self.encoder(h)\n","\n","        if self.vae_mode:\n","            mu, log_std = torch.chunk(z.contiguous().view(x.size(0), -1), 2, dim=1)\n","            std = torch.exp(log_std)\n","            z_dist = dist.Normal(mu, std)\n","            if reparam:\n","                z = z_dist.rsample()\n","            else:\n","                z = mu      \n","        else:\n","            # Estimate CPDs with autoregression\n","            z_dist = self.estimator(z)\n","\n","\n","        # Reconstruct x\n","        x_r = self.decoder(z)\n","        if self.out_sigmoid:\n","            x_r = nn.functional.sigmoid(x_r)\n","        x_r = x_r.view(-1, *self.input_shape)\n","\n","        return x_r, z, z_dist\n","\n","if __name__ == \"__main__\":\n","    model = LSAMOOD(input_shape=(1,256,256), code_length=64, cpd_channels=100, n_starting_features=4)\n","\n","\n","class BaseModule(nn.Module):\n","    \"\"\"\n","    Implements the basic module.\n","    All other modules inherit from this one\n","    \"\"\"\n","    def load_w(self, checkpoint_path):\n","        # type: (str) -> None\n","        \"\"\"\n","        Loads a checkpoint into the state_dict.\n","\n","        :param checkpoint_path: the checkpoint file to be loaded.\n","        \"\"\"\n","        self.load_state_dict(torch.load(checkpoint_path))\n","\n","    def __repr__(self):\n","        # type: () -> str\n","        \"\"\"\n","        String representation\n","        \"\"\"\n","        good_old = super(BaseModule, self).__repr__()\n","        addition = 'Total number of parameters: {:,}'.format(self.n_parameters)\n","\n","        return good_old + '\\n' + addition\n","\n","    def __call__(self, *args, **kwargs):\n","        return super(BaseModule, self).__call__(*args, **kwargs)\n","\n","    @property\n","    def n_parameters(self):\n","        # type: () -> int\n","        \"\"\"\n","        Number of parameters of the model.\n","        \"\"\"\n","        n_parameters = 0\n","        for p in self.parameters():\n","            if hasattr(p, 'mask'):\n","                n_parameters += torch.sum(p.mask).item()\n","            else:\n","                n_parameters += reduce(mul, p.shape)\n","        return int(n_parameters)\n","\n","def residual_op(x, functions, bns, activation_fn):\n","    # type: (torch.Tensor, List[Module, Module, Module], List[Module, Module, Module], Module) -> torch.Tensor\n","    \"\"\"\n","    Implements a global residual operation.\n","\n","    :param x: the input tensor.\n","    :param functions: a list of functions (nn.Modules).\n","    :param bns: a list of optional batch-norm layers.\n","    :param activation_fn: the activation to be applied.\n","    :return: the output of the residual operation.\n","    \"\"\"\n","    f1, f2, f3 = functions\n","    bn1, bn2, bn3 = bns\n","\n","    assert len(functions) == len(bns) == 3\n","    assert f1 is not None and f2 is not None\n","    assert not (f3 is None and bn3 is not None)\n","\n","    # A-branch\n","    ha = x\n","    ha = f1(ha)\n","    if bn1 is not None:\n","        ha = bn1(ha)\n","    ha = activation_fn(ha)\n","\n","    ha = f2(ha)\n","    if bn2 is not None:\n","        ha = bn2(ha)\n","\n","    # B-branch\n","    hb = x\n","    if f3 is not None:\n","        hb = f3(hb)\n","    if bn3 is not None:\n","        hb = bn3(hb)\n","\n","    # Residual connection\n","    out = ha + hb\n","    return activation_fn(out)\n","\n","\n","class BaseBlock(BaseModule):\n","    \"\"\" Base class for all blocks. \"\"\"\n","    def __init__(self, channel_in, channel_out, activation_fn, use_bn=True, use_bias=False):\n","        # type: (int, int, Module, bool, bool) -> None\n","        \"\"\"\n","        Class constructor.\n","\n","        :param channel_in: number of input channels.\n","        :param channel_out: number of output channels.\n","        :param activation_fn: activation to be employed.\n","        :param use_bn: whether or not to use batch-norm.\n","        :param use_bias: whether or not to use bias.\n","        \"\"\"\n","        super(BaseBlock, self).__init__()\n","\n","        assert not (use_bn and use_bias), 'Using bias=True with batch_normalization is forbidden.'\n","\n","        self._channel_in = channel_in\n","        self._channel_out = channel_out\n","        self._activation_fn = activation_fn\n","        self._use_bn = use_bn\n","        self._bias = use_bias\n","\n","    def get_bn(self):\n","        # type: () -> Optional[Module]\n","        \"\"\"\n","        Returns batch norm layers, if needed.\n","        :return: batch norm layers or None\n","        \"\"\"\n","        return nn.BatchNorm2d(num_features=self._channel_out) if self._use_bn else None\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Abstract forward function. Not implemented.\n","        \"\"\"\n","        raise NotImplementedError\n","\n","\n","class DownsampleBlock(BaseBlock):\n","    \"\"\" Implements a Downsampling block for images (Fig. 1ii). \"\"\"\n","    def __init__(self, channel_in, channel_out, activation_fn, use_bn=True, use_bias=False):\n","        # type: (int, int, Module, bool, bool) -> None\n","        \"\"\"\n","        Class constructor.\n","\n","        :param channel_in: number of input channels.\n","        :param channel_out: number of output channels.\n","        :param activation_fn: activation to be employed.\n","        :param use_bn: whether or not to use batch-norm.\n","        :param use_bias: whether or not to use bias.\n","        \"\"\"\n","        super(DownsampleBlock, self).__init__(channel_in, channel_out, activation_fn, use_bn, use_bias)\n","\n","        # Convolutions\n","        self.conv1a = nn.Conv2d(in_channels=channel_in, out_channels=channel_out, kernel_size=3,\n","                                padding=1, stride=2, bias=use_bias)\n","        self.conv1b = nn.Conv2d(in_channels=channel_out, out_channels=channel_out, kernel_size=3,\n","                                padding=1, stride=1, bias=use_bias)\n","        self.conv2a = nn.Conv2d(in_channels=channel_in, out_channels=channel_out, kernel_size=1,\n","                                padding=0, stride=2, bias=use_bias)\n","\n","        # Batch Normalization layers\n","        self.bn1a = self.get_bn()\n","        self.bn1b = self.get_bn()\n","        self.bn2a = self.get_bn()\n","\n","    def forward(self, x):\n","        # type: (torch.Tensor) -> torch.Tensor\n","        \"\"\"\n","        Forward propagation.\n","        :param x: the input tensor\n","        :return: the output tensor\n","        \"\"\"\n","        return residual_op(\n","            x,\n","            functions=[self.conv1a, self.conv1b, self.conv2a],\n","            bns=[self.bn1a, self.bn1b, self.bn2a],\n","            activation_fn=self._activation_fn\n","        )\n","\n","\n","class UpsampleBlock(BaseBlock):\n","    \"\"\" Implements a Upsampling block for images (Fig. 1ii). \"\"\"\n","    def __init__(self, channel_in, channel_out, activation_fn, use_bn=True, use_bias=False):\n","        # type: (int, int, Module, bool, bool) -> None\n","        \"\"\"\n","        Class constructor.\n","\n","        :param channel_in: number of input channels.\n","        :param channel_out: number of output channels.\n","        :param activation_fn: activation to be employed.\n","        :param use_bn: whether or not to use batch-norm.\n","        :param use_bias: whether or not to use bias.\n","        \"\"\"\n","        super(UpsampleBlock, self).__init__(channel_in, channel_out, activation_fn, use_bn, use_bias)\n","\n","        # Convolutions\n","        self.conv1a = nn.ConvTranspose2d(channel_in, channel_out, kernel_size=5,\n","                                         padding=2, stride=2, output_padding=1, bias=use_bias)\n","        self.conv1b = nn.Conv2d(in_channels=channel_out, out_channels=channel_out, kernel_size=3,\n","                                padding=1, stride=1, bias=use_bias)\n","        self.conv2a = nn.ConvTranspose2d(channel_in, channel_out, kernel_size=1,\n","                                         padding=0, stride=2, output_padding=1, bias=use_bias)\n","\n","        # Batch Normalization layers\n","        self.bn1a = self.get_bn()\n","        self.bn1b = self.get_bn()\n","        self.bn2a = self.get_bn()\n","\n","    def forward(self, x):\n","        # type: (torch.Tensor) -> torch.Tensor\n","        \"\"\"\n","        Forward propagation.\n","        :param x: the input tensor\n","        :return: the output tensor\n","        \"\"\"\n","        return residual_op(\n","            x,\n","            functions=[self.conv1a, self.conv1b, self.conv2a],\n","            bns=[self.bn1a, self.bn1b, self.bn2a],\n","            activation_fn=self._activation_fn\n","        )\n","\n","\n","class ResidualBlock(BaseBlock):\n","    \"\"\" Implements a Residual block for images (Fig. 1ii). \"\"\"\n","    def __init__(self, channel_in, channel_out, activation_fn, use_bn=True, use_bias=False):\n","        # type: (int, int, Module, bool, bool) -> None\n","        \"\"\"\n","        Class constructor.\n","\n","        :param channel_in: number of input channels.\n","        :param channel_out: number of output channels.\n","        :param activation_fn: activation to be employed.\n","        :param use_bn: whether or not to use batch-norm.\n","        :param use_bias: whether or not to use bias.\n","        \"\"\"\n","        super(ResidualBlock, self).__init__(channel_in, channel_out, activation_fn, use_bn, use_bias)\n","\n","        # Convolutions\n","        self.conv1 = nn.Conv2d(in_channels=channel_in, out_channels=channel_out, kernel_size=3,\n","                               padding=1, stride=1, bias=use_bias)\n","        self.conv2 = nn.Conv2d(in_channels=channel_out, out_channels=channel_out, kernel_size=3,\n","                               padding=1, stride=1, bias=use_bias)\n","\n","        # Batch Normalization layers\n","        self.bn1 = self.get_bn()\n","        self.bn2 = self.get_bn()\n","\n","    def forward(self, x):\n","        # type: (torch.Tensor) -> torch.Tensor\n","        \"\"\"\n","        Forward propagation.\n","        :param x: the input tensor\n","        :return: the output tensor\n","        \"\"\"\n","        return residual_op(\n","            x,\n","            functions=[self.conv1, self.conv2, None],\n","            bns=[self.bn1, self.bn2, None],\n","            activation_fn=self._activation_fn\n","        )\n","\n","class MaskedFullyConnection(BaseModule, nn.Linear):\n","    \"\"\"\n","    Implements a Masked Fully Connection layer (MFC, Eq. 6).\n","    This is the autoregressive layer employed for the estimation of\n","    densities of image feature vectors.\n","    \"\"\"\n","    def __init__(self, mask_type, in_channels, out_channels, *args, **kwargs):\n","        \"\"\"\n","        Class constructor.\n","\n","        :param mask_type: type of autoregressive layer, either `A` or `B`.\n","        :param in_channels: number of input channels.\n","        :param out_channels: number of output channels.\n","        \"\"\"\n","        self.mask_type = mask_type\n","        self.in_channels = in_channels\n","        self.out_channels = out_channels\n","        super(MaskedFullyConnection, self).__init__(*args, **kwargs)\n","\n","        assert mask_type in ['A', 'B']\n","        self.register_buffer('mask', self.weight.data.clone())\n","\n","        # Build mask\n","        self.mask.fill_(0)\n","        for f in range(0 if mask_type == 'B' else 1, self.out_features // self.out_channels):\n","            start_row = f*self.out_channels\n","            end_row = (f+1)*self.out_channels\n","            start_col = 0\n","            end_col = f*self.in_channels if mask_type == 'A' else (f+1)*self.in_channels\n","            if start_col != end_col:\n","                self.mask[start_row:end_row, start_col:end_col] = 1\n","\n","        self.weight.mask = self.mask\n","\n","    def forward(self, x):\n","        # type: (torch.Tensor) -> torch.Tensor\n","        \"\"\"\n","        Forward propagation.\n","\n","        :param x: the input tensor.\n","        :return: the output of a MFC manipulation.\n","        \"\"\"\n","\n","        # Reshape\n","        x = torch.transpose(x, 1, 2).contiguous()\n","        x = x.view(len(x), -1)\n","\n","        # Mask weights and call fully connection\n","        self.weight.data *= self.mask\n","        o = super(MaskedFullyConnection, self).forward(x)\n","\n","        # Reshape again\n","        o = o.view(len(o), -1, self.out_channels)\n","        o = torch.transpose(o, 1, 2).contiguous()\n","\n","        return o\n","\n","    def __repr__(self):\n","        # type: () -> str\n","        \"\"\"\n","        String representation.\n","        \"\"\"\n","        return self.__class__.__name__ + '(' \\\n","               + 'mask_type=' + str(self.mask_type) \\\n","               + ', in_features=' + str(self.in_features // self.in_channels) \\\n","               + ', out_features=' + str(self.out_features // self.out_channels)\\\n","               + ', in_channels=' + str(self.in_channels) \\\n","               + ', out_channels=' + str(self.out_channels) \\\n","               + ', n_params=' + str(self.n_parameters) + ')'\n","\n","\n","class Estimator1D(BaseModule):\n","    \"\"\"\n","    Implements an estimator for 1-dimensional vectors.\n","    1-dimensional vectors arise from the encoding of images.\n","    This module is employed in MNIST and CIFAR10 LSA models.\n","    Takes as input a latent vector and outputs cpds for each variable.\n","    \"\"\"\n","    def __init__(self, code_length, fm_list, cpd_channels):\n","        # type: (int, List[int], int) -> None\n","        \"\"\"\n","        Class constructor.\n","\n","        :param code_length: the dimensionality of latent vectors.\n","        :param fm_list: list of channels for each MFC layer.\n","        :param cpd_channels: number of bins in which the multinomial works.\n","        \"\"\"\n","        super(Estimator1D, self).__init__()\n","\n","        self.code_length = code_length\n","        self.fm_list = fm_list\n","        self.cpd_channels = cpd_channels\n","\n","        activation_fn = nn.LeakyReLU()\n","\n","        # Add autoregressive layers\n","        layers_list = []\n","        mask_type = 'A'\n","        fm_in = 1\n","        for l in range(0, len(fm_list)):\n","\n","            fm_out = fm_list[l]\n","            layers_list.append(\n","                MaskedFullyConnection(mask_type=mask_type,\n","                                      in_features=fm_in * code_length,\n","                                      out_features=fm_out * code_length,\n","                                      in_channels=fm_in, out_channels=fm_out)\n","            )\n","            layers_list.append(activation_fn)\n","\n","            mask_type = 'B'\n","            fm_in = fm_list[l]\n","\n","        # Add final layer providing cpd params\n","        layers_list.append(\n","            MaskedFullyConnection(mask_type=mask_type,\n","                                  in_features=fm_in * code_length,\n","                                  out_features=cpd_channels * code_length,\n","                                  in_channels=fm_in,\n","                                  out_channels=cpd_channels))\n","\n","        self.layers = nn.Sequential(*layers_list)\n","\n","    def forward(self, x):\n","        # type: (torch.Tensor) -> torch.Tensor\n","        \"\"\"\n","        Forward propagation.\n","\n","        :param x: the batch of latent vectors.\n","        :return: the batch of CPD estimates.\n","        \"\"\"\n","        h = torch.unsqueeze(x, dim=1)  # add singleton channel dim\n","        h = self.layers(h)\n","        o = h\n","\n","        return o\n","\n","def get_square_mask(data_shape, square_size, n_squares, noise_val=(0, 0), channel_wise_n_val=False, square_pos=None):\n","    \"\"\"Returns a 'mask' with the same size as the data, where random squares are != 0\n","\n","    Args:\n","        data_shape ([tensor]): [data_shape to determine the shape of the returned tensor]\n","        square_size ([tuple]): [int/ int tuple (min_size, max_size), determining the min and max squear size]\n","        n_squares ([type]): [int/ int tuple (min_number, max_number), determining the min and max number of squares]\n","        noise_val (tuple, optional): [int/ int tuple (min_val, max_val), determining the min and max value given in the \n","                                        squares, which habe the value != 0 ]. Defaults to (0, 0).\n","        channel_wise_n_val (bool, optional): [Use a different value for each channel]. Defaults to False.\n","        square_pos ([type], optional): [Square position]. Defaults to None.\n","    \"\"\"\n","\n","    def mask_random_square(img_shape, square_size, n_val, channel_wise_n_val=False, square_pos=None):\n","        \"\"\"Masks (sets = 0) a random square in an image\"\"\"\n","\n","        img_h = img_shape[-2]\n","        img_w = img_shape[-1]\n","\n","        img = np.zeros(img_shape)\n","\n","        if square_pos is None:\n","            w_start = np.random.randint(0, img_w - square_size)\n","            h_start = np.random.randint(0, img_h - square_size)\n","        else:\n","            pos_wh = square_pos[np.random.randint(0, len(square_pos))]\n","            w_start = pos_wh[0]\n","            h_start = pos_wh[1]\n","\n","        if img.ndim == 2:\n","            rnd_n_val = get_range_val(n_val)\n","            img[h_start : (h_start + square_size), w_start : (w_start + square_size)] = rnd_n_val\n","        elif img.ndim == 3:\n","            if channel_wise_n_val:\n","                for i in range(img.shape[0]):\n","                    rnd_n_val = get_range_val(n_val)\n","                    img[i, h_start : (h_start + square_size), w_start : (w_start + square_size)] = rnd_n_val\n","            else:\n","                rnd_n_val = get_range_val(n_val)\n","                img[:, h_start : (h_start + square_size), w_start : (w_start + square_size)] = rnd_n_val\n","        elif img.ndim == 4:\n","            if channel_wise_n_val:\n","                for i in range(img.shape[0]):\n","                    rnd_n_val = get_range_val(n_val)\n","                    img[:, i, h_start : (h_start + square_size), w_start : (w_start + square_size)] = rnd_n_val\n","            else:\n","                rnd_n_val = get_range_val(n_val)\n","                img[:, :, h_start : (h_start + square_size), w_start : (w_start + square_size)] = rnd_n_val\n","\n","        return img\n","\n","    def mask_random_squares(img_shape, square_size, n_squares, n_val, channel_wise_n_val=False, square_pos=None):\n","        \"\"\"Masks a given number of squares in an image\"\"\"\n","        img = np.zeros(img_shape)\n","        for i in range(n_squares):\n","            img = mask_random_square(\n","                img_shape, square_size, n_val, channel_wise_n_val=channel_wise_n_val, square_pos=square_pos\n","            )\n","        return img\n","\n","    ret_data = np.zeros(data_shape)\n","    for sample_idx in range(data_shape[0]):\n","        # rnd_n_val = get_range_val(noise_val)\n","        rnd_square_size = get_range_val(square_size)\n","        rnd_n_squares = get_range_val(n_squares)\n","\n","        ret_data[sample_idx] = mask_random_squares(\n","            data_shape[1:],\n","            square_size=rnd_square_size,\n","            n_squares=rnd_n_squares,\n","            n_val=noise_val,\n","            channel_wise_n_val=channel_wise_n_val,\n","            square_pos=square_pos,\n","        )\n","\n","    return ret_data\n","\n","def get_range_val(value, rnd_type=\"uniform\"):\n","    if isinstance(value, (list, tuple, np.ndarray)):\n","        if len(value) == 2:\n","            if value[0] == value[1]:\n","                n_val = value[0]\n","            else:\n","                orig_type = type(value[0])\n","                if rnd_type == \"uniform\":\n","                    n_val = random.uniform(value[0], value[1])\n","                elif rnd_type == \"normal\":\n","                    n_val = random.normalvariate(value[0], value[1])\n","                n_val = orig_type(n_val)\n","        elif len(value) == 1:\n","            n_val = value[0]\n","        else:\n","            raise RuntimeError(\"value must be either a single vlaue or a list/tuple of len 2\")\n","        return n_val\n","    else:\n","        return value\n","\n","\n","class Encoder(BaseModule):\n","    \"\"\"\n","    MOOD model encoder based on CIFAR10.\n","    \"\"\"\n","    def __init__(self, input_shape, code_length, n_starting_features=32, conv=nn.Conv2d):\n","        # type: (Tuple[int, int, int], int) -> None\n","        \"\"\"\n","        Class constructor:\n","\n","        :param input_shape: the shape of CIFAR10 samples.\n","        :param code_length: the dimensionality of latent vectors.\n","        \"\"\"\n","        super(Encoder, self).__init__()\n","\n","        self.input_shape = input_shape\n","        self.code_length = code_length\n","\n","        if len(input_shape) == 3:\n","          c, h, w = input_shape\n","          self.deepest_shape = (n_starting_features*8, h // 8, w // 8)\n","          \n","\n","        activation_fn = nn.LeakyReLU()\n","\n","        # Convolutional network\n","        self.conv = nn.Sequential(\n","            conv(in_channels=c, out_channels=n_starting_features, kernel_size=3, bias=False),\n","            activation_fn,\n","            ResidualBlock(channel_in=n_starting_features, channel_out=n_starting_features, activation_fn=activation_fn),\n","            DownsampleBlock(channel_in=n_starting_features, channel_out=n_starting_features*2, activation_fn=activation_fn),\n","            DownsampleBlock(channel_in=n_starting_features*2, channel_out=n_starting_features*4, activation_fn=activation_fn),\n","            DownsampleBlock(channel_in=n_starting_features*4, channel_out=n_starting_features*8, activation_fn=activation_fn),\n","        )\n","\n","        # FC network\n","        self.fc = nn.Sequential(\n","            nn.Linear(in_features=reduce(mul, self.deepest_shape), out_features=n_starting_features*8),\n","            nn.BatchNorm1d(num_features=n_starting_features*8),\n","            activation_fn,\n","            nn.Linear(in_features=n_starting_features*8, out_features=code_length),\n","            nn.Sigmoid()\n","        )\n","    def forward(self, x):\n","        # types: (torch.Tensor) -> torch.Tensor\n","        \"\"\"\n","        Forward propagation.\n","\n","        :param x: the input batch of images.\n","        :return: the batch of latent vectors.\n","        \"\"\"\n","        h = x\n","        h = self.conv(h)\n","        h = h.view(len(h), -1)\n","        o = self.fc(h)\n","\n","        return o\n","\n","\n","class Decoder(BaseModule):\n","    \"\"\"\n","    CIFAR10 model decoder.\n","    \"\"\"\n","    def __init__(self, code_length, deepest_shape, output_shape, n_starting_features=32, n_channels=3, conv=nn.Conv3d):\n","        # type: (int, Tuple[int, int, int], Tuple[int, int, int]) -> None\n","        \"\"\"\n","        Class constructor.\n","\n","        :param code_length: the dimensionality of latent vectors.\n","        :param deepest_shape: the dimensionality of the encoder's deepest convolutional map.\n","        :param output_shape: the shape of CIFAR10 samples.\n","        \"\"\"\n","        super(Decoder, self).__init__()\n","\n","        self.code_length = code_length\n","        self.deepest_shape = deepest_shape\n","        self.output_shape = output_shape\n","\n","        activation_fn = nn.LeakyReLU()\n","        \n","\n","\n","        # FC network\n","        self.fc = nn.Sequential(\n","            nn.Linear(in_features=code_length, out_features=n_starting_features*8),\n","            nn.BatchNorm1d(num_features=n_starting_features*8),\n","            activation_fn,\n","            nn.Linear(in_features=n_starting_features*8, out_features=reduce(mul, deepest_shape)),\n","            nn.BatchNorm1d(num_features=reduce(mul, deepest_shape)),\n","            activation_fn\n","        )\n","\n","        # Convolutional network\n","        self.conv = nn.Sequential(\n","            UpsampleBlock(channel_in=n_starting_features*8, channel_out=n_starting_features*4, activation_fn=activation_fn),\n","            UpsampleBlock(channel_in=n_starting_features*4, channel_out=n_starting_features*2, activation_fn=activation_fn),\n","            UpsampleBlock(channel_in=n_starting_features*2, channel_out=n_starting_features, activation_fn=activation_fn),\n","            ResidualBlock(channel_in=n_starting_features, channel_out=n_starting_features, activation_fn=activation_fn),\n","            conv(in_channels=n_starting_features, out_channels=n_channels, kernel_size=1, bias=False)\n","        )\n","\n","    def forward(self, x):\n","        # types: (torch.Tensor) -> torch.Tensor\n","        \"\"\"\n","        Forward propagation.\n","\n","        :param x: the batch of latent vectors.\n","        :return: the batch of reconstructions.\n","        \"\"\"\n","        h = x\n","        h = self.fc(h)\n","        h = h.view(len(h), *self.deepest_shape)\n","        h = self.conv(h)\n","        o = h\n","\n","        return o\n","\n","\n","class LSAMOOD(BaseModule):\n","    \"\"\"\n","    LSA model for CIFAR10 one-class classification.\n","    \"\"\"\n","    def __init__(self,  input_shape, code_length, cpd_channels, d=3, n_starting_features=32, n_channels=3, out_sigmoid=True, vae_mode=False):\n","        # type: (Tuple[int, int, int], int, int) -> None\n","        \"\"\"\n","        Class constructor.\n","\n","        :param input_shape: the shape of CIFAR10 samples.\n","        :param code_length: the dimensionality of latent vectors.\n","        :param cpd_channels: number of bins in which the multinomial works.\n","        \"\"\"\n","        super(LSAMOOD, self).__init__()\n","\n","        if type(input_shape) is tuple:\n","            d=len(input_shape)-1\n","\n","        self.input_shape = input_shape\n","        self.code_length = code_length\n","        self.out_sigmoid = out_sigmoid\n","        self.vae_mode = vae_mode\n","        \n","        self.d = d\n","        conv = nn.Conv2d\n","        \n","\n","        # Build encoder\n","        self.encoder = Encoder(\n","            input_shape=input_shape,\n","            code_length=code_length*2 if vae_mode else code_length,\n","            n_starting_features=n_starting_features,\n","            conv=conv\n","        )\n","\n","        # Build decoder\n","        self.decoder = Decoder(\n","            code_length=code_length,\n","            deepest_shape=self.encoder.deepest_shape,\n","            output_shape=input_shape,\n","            n_starting_features=n_starting_features,\n","            n_channels=n_channels,\n","            conv=conv\n","        )\n","\n","        if not vae_mode:\n","            # Build estimator\n","            self.estimator = Estimator1D(\n","                code_length=code_length,\n","                fm_list=[n_starting_features, n_starting_features, n_starting_features, n_starting_features],\n","                cpd_channels=cpd_channels\n","            )\n","\n","    def forward(self, x, reparam=True):\n","        # type: (torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]\n","        \"\"\"\n","        Forward propagation.\n","\n","        :param x: the input batch of images.\n","        :return: a tuple of torch.Tensors holding reconstructions, latent vectors and CPD estimates.\n","        \"\"\"\n","        h = x\n","\n","        # Produce representations\n","        z = self.encoder(h)\n","\n","        if self.vae_mode:\n","            mu, log_std = torch.chunk(z.contiguous().view(x.size(0), -1), 2, dim=1)\n","            std = torch.exp(log_std)\n","            z_dist = dist.Normal(mu, std)\n","            if reparam:\n","                z = z_dist.rsample()\n","            else:\n","                z = mu      \n","        else:\n","            # Estimate CPDs with autoregression\n","            z_dist = self.estimator(z)\n","\n","\n","        # Reconstruct x\n","        x_r = self.decoder(z)\n","        if self.out_sigmoid:\n","            x_r = nn.functional.sigmoid(x_r)\n","        x_r = x_r.view(-1, *self.input_shape)\n","\n","        return x_r, z, z_dist\n","\n","if __name__ == \"__main__\":\n","    model = LSAMOOD(input_shape=(1,256,256), code_length=64, cpd_channels=100, n_starting_features=4)\n","    \n","\n"," class LSALoss(nn.Module):\n","    \"\"\"\n","    Implements the loss of a LSA model.\n","    It is a sum of the reconstruction loss and the autoregression loss.\n","    \"\"\"\n","    def __init__(self, cpd_channels, lam=1, use_mse_reco=False, mean_results=True):\n","        # type: (int, float) -> None\n","        \"\"\"\n","        Class constructor.\n","\n","        :param cpd_channels: number of bins in which the multinomial works.\n","        :param lam: weight of the autoregression loss.\n","        \"\"\"\n","        super(LSALoss, self).__init__()\n","\n","        self.cpd_channels = cpd_channels\n","        self.lam = lam\n","\n","        # Set up loss modules\n","        if use_mse_reco:\n","            self.reconstruction_loss_fn = nn.MSELoss()\n","        else:\n","            self.reconstruction_loss_fn = ReconstructionLoss(mean_results=mean_results)\n","        self.autoregression_loss_fn = AutoregressionLoss(cpd_channels, mean_results=mean_results)\n","\n","        # Numerical variables\n","        self.reconstruction_loss = None\n","        self.autoregression_loss = None\n","        self.total_loss = None\n","\n","        self.mean_results = mean_results\n","\n","    def forward(self, x, x_r, z, z_dist):\n","        # type: (torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor) -> torch.Tensor\n","        \"\"\"\n","        Forward propagation.\n","\n","        :param x: the batch of input samples.\n","        :param x_r: the batch of reconstructions.\n","        :param z: the batch of latent representations.\n","        :param z_dist: the batch of estimated cpds.\n","        :return: the loss of the model (averaged along the batch axis).\n","        \"\"\"\n","        # Compute pytorch loss\n","        rec_loss = self.reconstruction_loss_fn(x, x_r)\n","        arg_loss = self.autoregression_loss_fn(z, z_dist)\n","        tot_loss = rec_loss + self.lam * arg_loss\n","\n","        # Store numerical\n","        if self.mean_results:\n","            self.reconstruction_loss = rec_loss.item()\n","            self.autoregression_loss = arg_loss.item()\n","            self.total_loss = tot_loss.item()\n","        else:\n","            self.reconstruction_loss = rec_loss\n","            self.autoregression_loss = arg_loss\n","            self.total_loss = tot_loss\n","\n","\n","        return tot_loss\n","\n","\n","\n","class ReconstructionLoss(BaseModule):\n","    \"\"\"\n","    Implements the reconstruction loss.\n","    \"\"\"\n","    def __init__(self, mean_results=True):\n","        # type: () -> None\n","        \"\"\"\n","        Class constructor.\n","        \"\"\"\n","        super(ReconstructionLoss, self).__init__()\n","        self.mean_results = mean_results\n","\n","    def forward(self, x, x_r):\n","        # type: (torch.Tensor, torch.Tensor) -> torch.Tensor\n","        \"\"\"\n","        Forward propagation.\n","\n","        :param x: the batch of input samples.\n","        :param x_r: the batch of reconstructions.\n","        :return: the mean reconstruction loss (averaged along the batch axis).\n","        \"\"\"\n","        L = torch.pow((x - x_r), 2)\n","\n","        while L.dim() > 1:\n","            L = torch.sum(L, dim=-1)\n","\n","        if self.mean_results:\n","            return torch.mean(L)\n","        else:\n","            return L\n","\n","\n","class AutoregressionLoss(BaseModule):\n","    \"\"\"\n","    Implements the autoregression loss.\n","    Given a representation and the estimated cpds, provides\n","    the log-likelihood of the representation under the estimated prior.\n","    \"\"\"\n","    def __init__(self, cpd_channels, mean_results=True):\n","\n","        # type: (int) -> None\n","        \"\"\"\n","        Class constructor.\n","\n","        :param cpd_channels: number of bins in which the multinomial works.\n","        \"\"\"\n","        super(AutoregressionLoss, self).__init__()\n","\n","        self.cpd_channels = cpd_channels\n","        self.mean_results = mean_results\n","\n","        # Avoid nans\n","        self.eps = np.finfo(float).eps\n","\n","    def forward(self, z, z_dist):\n","        # type: (torch.Tensor, torch.Tensor) -> torch.Tensor\n","        \"\"\"\n","        Forward propagation.\n","\n","        :param z: the batch of latent representations.\n","        :param z_dist: the batch of estimated cpds.\n","        :return: the mean log-likelihood (averaged along the batch axis).\n","        \"\"\"\n","        z_d = z.detach()\n","\n","        # Apply softmax\n","        z_dist = F.softmax(z_dist, dim=1)\n","\n","        # Flatten out codes and distributions\n","        z_d = z_d.view(len(z_d), -1).contiguous()\n","        z_dist = z_dist.view(len(z_d), self.cpd_channels, -1).contiguous()\n","\n","        # Log (regularized), pick the right ones\n","        z_dist = torch.clamp(z_dist, self.eps, 1 - self.eps)\n","        log_z_dist = torch.log(z_dist)\n","        index = torch.clamp(torch.unsqueeze(z_d, dim=1) * self.cpd_channels, min=0,\n","                            max=(self.cpd_channels - 1)).long()\n","        selected = torch.gather(log_z_dist, dim=1, index=index)\n","        selected = torch.squeeze(selected, dim=1)\n","\n","        # Sum and mean\n","        S = torch.sum(selected, dim=-1)\n","\n","        if self.mean_results:\n","            nll = - torch.mean(S)\n","        else:\n","            nll = - S\n","\n","        return nll\n","\n","\n","def kl_loss_fn(z_post, sum_samples=True, correct=False, sumdim=(1,2,3)):\n","    z_prior = dist.Normal(0, 1.0)\n","    kl_div = dist.kl_divergence(z_post, z_prior)\n","    if correct:\n","        kl_div = torch.sum(kl_div, dim=sumdim)\n","    else:\n","        kl_div = torch.mean(kl_div, dim=sumdim)\n","    if sum_samples:\n","        return torch.mean(kl_div)\n","    else:\n","        return kl_div\n","\n","def rec_loss_fn(recon_x, x, sum_samples=True, correct=False, sumdim=(1,2,3)):\n","    if correct:\n","        x_dist = dist.Laplace(recon_x, 1.0)\n","        log_p_x_z = x_dist.log_prob(x)\n","        log_p_x_z = torch.sum(log_p_x_z, dim=sumdim)\n","    else:\n","        log_p_x_z = -torch.abs(recon_x - x)\n","        log_p_x_z = torch.mean(log_p_x_z, dim=sumdim)\n","    if sum_samples:\n","        return -torch.mean(log_p_x_z)\n","    else:\n","        return -log_p_x_z\n","\n","def geco_beta_update(beta, error_ema, goal, step_size, min_clamp=1e-10, max_clamp=1e4, speedup=None):\n","    constraint = (error_ema - goal).detach()\n","    if speedup is not None and constraint > 0.0:\n","        beta = beta * torch.exp(speedup * step_size * constraint)\n","    else:\n","        beta = beta * torch.exp(step_size * constraint)\n","    if min_clamp is not None:\n","        beta = np.max((beta.item(), min_clamp))\n","    if max_clamp is not None:\n","        beta = np.min((beta.item(), max_clamp))\n","    return beta\n","\n","def get_ema(new, old, alpha):\n","    if old is None:\n","        return new\n","    return (1.0 - alpha) * new + alpha * old\n","\n","#### Scoring methods\n","class VAEScorer():\n","\n","    def __init__(self, model, model_loss_forward=None, model_forward=None, input_shape=(256,256,256), beta=0.01, theta=1, sumdim_kl=(1,2,3), sumdim_rec=(1,2,3)):\n","        self.input_shape = input_shape\n","        self.beta = beta\n","        self.theta = theta\n","        self.model = model\n","        self.model_forward = model_forward\n","        self.sumdim_kl = sumdim_kl\n","        self.sumdim_rec = sumdim_rec\n","        if model_loss_forward is None:\n","            self.model_loss_forward = self.default_model_loss_forward\n","        else:\n","            self.model_loss_forward = model_loss_forward\n","\n","    def normalize(self, tensor):\n","        tens_deta = tensor.detach().cpu()\n","        minv = np.min(tens_deta.numpy())\n","        maxv = np.max(tens_deta.numpy())\n","        tens_deta -= float(minv)\n","        if maxv != 0:\n","            tens_deta /= float(maxv)\n","        return tens_deta\n","\n","    def default_model_loss_forward(self, inputs, sum_samples=True, calc_loss=True):\n","        x_r, z_dist = self.model_forward(inputs)\n","        if calc_loss:\n","            kl_loss_ = kl_loss_fn(z_dist, sum_samples=sum_samples, sumdim=self.sumdim_kl)\n","            rec_loss_ = rec_loss_fn(x_r, inputs, sum_samples=sum_samples, sumdim=self.sumdim_rec)\n","            loss_ = kl_loss_ * self.beta + rec_loss_ * self.theta\n","            return x_r, loss_, kl_loss_, rec_loss_ #recon, totalloss, regloss, recoloss\n","        else:\n","            return x_r\n","    \n","    def score_image(self, inputs, return_reco=False):\n","        x_r, image_scores, _, _ = self.model_loss_forward(inputs, sum_samples=False)\n","        if return_reco:\n","            return image_scores.detach().cpu(), x_r.detach().cpu()\n","        else:\n","            return image_scores.detach().cpu()\n","\n","    def score_pixels(self, inputs, grad_type=\"vanilla\", n_runs=2, smoothing_kernel=8, pixel_score_mode='combi'):\n","        x_r = self.model_loss_forward(inputs, calc_loss=False)\n","\n","        if pixel_score_mode == \"combi\":\n","            rec = torch.pow((x_r - inputs), 2).detach().cpu()\n","            rec = torch.mean(rec, dim=1, keepdim=True)\n","\n","            def __err_fn(x):\n","                x_r, _, kl_loss_, _ = self.model_loss_forward(x)\n","                return kl_loss_\n","\n","            loss_grad_kl = (\n","                get_smooth_image_gradient(\n","                    model=self.model, inpt=inputs, err_fn=__err_fn, grad_type=grad_type, n_runs=n_runs\n","                )\n","                .detach()\n","                .cpu()\n","            )\n","            loss_grad_kl = torch.mean(loss_grad_kl, dim=1, keepdim=True)\n","\n","            pixel_scores = smooth_tensor(self.normalize(loss_grad_kl), kernel_size=smoothing_kernel) * rec\n","\n","        elif pixel_score_mode == \"rec\":\n","\n","            rec = torch.pow((x_r - inputs), 2).detach().cpu()\n","            rec = torch.mean(rec, dim=1, keepdim=True)\n","            pixel_scores = rec\n","\n","        elif pixel_score_mode == \"grad\":\n","\n","            def __err_fn(x):\n","                _, loss_, _, _ = self.model_loss_forward(x)\n","                return torch.mean(loss_)\n","\n","            loss_grad_kl = (\n","                get_smooth_image_gradient(\n","                    model=self.model, inpt=inputs, err_fn=__err_fn, grad_type=grad_type, n_runs=n_runs\n","                )\n","                .detach()\n","                .cpu()\n","            )\n","            loss_grad_kl = torch.mean(loss_grad_kl, dim=1, keepdim=True)\n","\n","            pixel_scores = smooth_tensor(self.normalize(loss_grad_kl), kernel_size=smoothing_kernel)\n","\n","        return pixel_scores.detach().cpu()\n","\n","###################\n","#From CEVAE - ends\n","\n","class PixelScorer(): #For non-VAE models, to score pixels\n","\n","    def __init__(self, model, model_loss_forward):\n","        self.model = model\n","        self.model_loss_forward = model_loss_forward\n","\n","    def normalize(self, tensor):\n","        tens_deta = tensor.detach().cpu()\n","        minv = np.min(tens_deta.numpy())\n","        maxv = np.max(tens_deta.numpy())\n","        tens_deta -= float(minv)\n","        if maxv != 0:\n","            tens_deta /= float(maxv)\n","        return tens_deta\n","\n","    def score_pixels(self, inputs, grad_type=\"vanilla\", n_runs=2, smoothing_kernel=8, pixel_score_mode='combi'):\n","        x_r = self.model_loss_forward(inputs, calc_loss=False)\n","\n","        if pixel_score_mode == \"combi\":\n","            rec = torch.pow((x_r - inputs), 2).detach().cpu()\n","            rec = torch.mean(rec, dim=1, keepdim=True)\n","\n","            def __err_fn(x):\n","                x_r, loss = self.model_loss_forward(x)\n","                return loss\n","\n","            loss_grad_kl = (\n","                get_smooth_image_gradient(\n","                    model=self.model, inpt=inputs, err_fn=__err_fn, grad_type=grad_type, n_runs=n_runs\n","                )\n","                .detach()\n","                .cpu()\n","            )\n","            loss_grad_kl = torch.mean(loss_grad_kl, dim=1, keepdim=True)\n","\n","            pixel_scores = smooth_tensor(self.normalize(loss_grad_kl), kernel_size=smoothing_kernel) * rec\n","\n","        elif pixel_score_mode == \"rec\":\n","\n","            rec = torch.pow((x_r - inputs), 2).detach().cpu()\n","            rec = torch.mean(rec, dim=1, keepdim=True)\n","            pixel_scores = rec\n","\n","        elif pixel_score_mode == \"grad\":\n","\n","            def __err_fn(x):\n","                x_r, loss = self.model_loss_forward(x)\n","                return loss\n","\n","            loss_grad_kl = (\n","                get_smooth_image_gradient(\n","                    model=self.model, inpt=inputs, err_fn=__err_fn, grad_type=grad_type, n_runs=n_runs\n","                )\n","                .detach()\n","                .cpu()\n","            )\n","            loss_grad_kl = torch.mean(loss_grad_kl, dim=1, keepdim=True)\n","\n","            pixel_scores = smooth_tensor(self.normalize(loss_grad_kl), kernel_size=smoothing_kernel)\n","\n","        return pixel_scores.detach().cpu()\n","\n","\n","checkpoint2load = None\n","\n","trainset = MoodTrainSet(indices=indicesOfImgVols, region=mood_region, data_path=imgsh5_train, lazypatch=True if patch_size else False, preload=preload_h5)\n","valset = MoodValSet(data_path=imgsh5_val, lazypatch=True if patch_size else False, preload=preload_h5)\n","    \n","if patch_size:\n","    input_shape = tuple(x for x in patch_size if x!=1)\n","    trainset = torchio.data.Queue(\n","                    subjects_dataset = trainset,\n","                    max_length = patchQ_len,\n","                    samples_per_volume = patches_per_volume,\n","                    sampler = torchio.data.UniformSampler(patch_size=patch_size),\n","                    # num_workers = num_workers\n","                )\n","    valset = torchio.data.Queue(\n","                    subjects_dataset = valset,\n","                    max_length = patchQ_len,\n","                    samples_per_volume = patches_per_volume,\n","                    sampler = torchio.data.UniformSampler(patch_size=patch_size),\n","                    # num_workers = num_workers\n","                )\n","\n","train_loader = DataLoader(dataset=trainset,batch_size=batch_size,shuffle=True, num_workers=num_workers)\n","val_loader = None if (valset is None) or (not do_val) else DataLoader(dataset=valset,batch_size=batch_size,shuffle=False, num_workers=num_workers)\n","\n","model = LSAMOOD(input_shape=(n_channels,)+input_shape, code_length=code_length, cpd_channels=cpd_channels, n_starting_features=n_starting_features, n_channels=n_channels, out_sigmoid=out_sigmoid, vae_mode=IsVAE)\n","model.to(device)\n","optimizer = Adam(model.parameters(), lr=lr)\n","\n","\n","if checkpoint2load:\n","  chk = torch.load(checkpoint2load)\n","  model.load_state_dict(chk['state_dict'])\n","  optimizer.load_state_dict(chk['optimizer'])\n","  amp.load_state_dict(chk['amp'])\n","  start_epoch = chk['epoch'] + 1\n","  best_loss = chk['loss'] \n","else:\n","  start_epoch = 0\n","  best_loss = float('inf')\n","\n","\n","for epoch in range(start_epoch, num_epochs):\n","        #Train\n","        model.train()\n","        runningLoss = 0.0\n","        runningLossCounter = 0.0\n","        train_loss = 0.0\n","        print('Epoch '+ str(epoch)+ ': Train')\n","        with tqdm(total=len(train_loader)) as pbar:\n","            for i, data in enumerate(train_loader):\n","                  img = data['img']['data'].squeeze(-1) #* 2 - 1 #For 2D cases\n","                  images = Variable(img).to(device) \n","                  optimizer.zero_grad()\n","                  ### VAE Part\n","                  loss_vae = 0\n","                  if ce_factor < 1:\n","                      x_r, _, z_dist = model(images)\n","\n","                      kl_loss = 0\n","                      if beta > 0:\n","                          if IsVAE:\n","                              kl_loss = kl_loss_fn(z_dist, sumdim=(1,)) * beta\n","                          else:\n","                              sys.exit(\"KLD Not gonna work\")\n","                              kl_loss = kl_loss_fn(z_dist, sumdim=(1,2)) * beta\n","                      if model.d == 3:\n","                          rec_loss_vae = rec_loss_fn(x_r, images, sumdim=(1,2,3,4))\n","                      else:\n","                          rec_loss_vae = rec_loss_fn(x_r, images, sumdim=(1,2,3))\n","                      loss_vae = kl_loss + rec_loss_vae * theta\n","\n","                  ### CE Part\n","                  loss_ce = 0\n","                  if ce_factor > 0:\n","\n","                      ce_tensor = get_square_mask(\n","                          (images.size(0), n_channels)+input_shape,\n","                          square_size=(0, np.max(input_shape) // 2),\n","                          noise_val=(torch.min(img).item(), torch.max(img).item()),\n","                          n_squares=(0, 3),\n","                      )\n","                      ce_tensor = torch.from_numpy(ce_tensor).float()\n","                      inpt_noisy = torch.where(ce_tensor != 0, ce_tensor, img)\n","\n","                      inpt_noisy = inpt_noisy.to(device)\n","                      x_rec_ce, _, _ = model(inpt_noisy)\n","                      if model.d == 3:\n","                          rec_loss_ce = rec_loss_fn(x_rec_ce, images, sumdim=(1,2,3,4))\n","                      else:\n","                          rec_loss_ce = rec_loss_fn(x_rec_ce, images, sumdim=(1,2,3))\n","                      loss_ce = rec_loss_ce\n","\n","                  loss = (1.0 - ce_factor) * loss_vae + ce_factor * loss_ce\n","\n","                  if use_geco and ce_factor < 1:\n","                      g_goal = 0.1\n","                      g_lr = 1e-4\n","                      vae_loss_ema = (1.0 - 0.9) * rec_loss_vae + 0.9 * vae_loss_ema\n","                      theta = geco_beta_update(theta, vae_loss_ema, g_goal, g_lr, speedup=2)\n","                      \n","                  if not torch.isfinite(loss):\n","                      logging.error('loss is not finite. Skipping the iteration.')\n","                      continue\n","                  optimizer.step()\n","                  loss = round(loss.data.item(),4)\n","                  train_loss += loss\n","                  print(loss)\n","                  runningLoss += loss\n","                  runningLossCounter += 1\n","                  logging.info('[%d/%d][%d/%d] Train Loss: %.4f' % ((epoch+1), num_epochs, i, len(train_loader), loss))\n","                  pbar.update(1)\n","        checkpoint = {\n","            'state_dict': model.state_dict(),\n","            'optimizer': optimizer.state_dict(),\n","        }\n","        torch.save(checkpoint, os.path.join(save_path, trainID+\".pth.tar\"))\n","\n","        #Validate\n","        if val_loader:\n","            model.eval()\n","            with torch.no_grad():  \n","                print('Epoch '+ str(epoch)+ ': Val')\n","                with tqdm(total=len(val_loader)) as pbar:              \n","                    for i, data in enumerate(val_loader):\n","                        try:\n","                            img = data['img']['data'].squeeze(-1) * 2 - 1 #For 2D cases\n","                            images = Variable(img).to(device) \n","                            if torch.equal(data['gt']['data'], data['img']['data']):\n","                                gt = torch.zeros(images.shape)\n","                            else:\n","                                gt = data['gt']['data']\n","\n","                            #y = [1 if gt_one.sum() > 0 else 0 for gt_one in gt]\n","                            \n","                            x_r, z, z_dist = model(images, reparam=False)\n","\n","                            kl_loss = 0\n","                            if beta > 0:\n","                                if IsVAE:\n","                                    kl_loss = kl_loss_fn(z_dist, sum_samples=False, sumdim=(1,)) * beta\n","                                else:\n","                                    sys.exit(\"KLD Not gonna work\")\n","                                    kl_loss = kl_loss_fn(z_dist, sum_samples=False, sumdim=(1,2)) * beta\n","                            if model.d == 3:\n","                                rec_loss_vae = rec_loss_fn(x_r, images, sum_samples=False, sumdim=(1,2,3,4))\n","                            else:\n","                                rec_loss_vae = rec_loss_fn(x_r, images, sum_samples=False, sumdim=(1,2,3))\n","                            loss = kl_loss + rec_loss_vae * theta\n","\n","                            logging.info('[%d/%d][%d/%d] Val Loss: %.4f' % ((epoch+1), num_epochs, i, len(val_loader), loss.mean().item()))\n","                        except Exception as ex:\n","                            logging.error(ex)\n","                        pbar.update(1)"],"execution_count":null,"outputs":[]}]}